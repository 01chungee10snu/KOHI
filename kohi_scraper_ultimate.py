import pandas as pd
import json
from playwright.sync_api import sync_playwright
import time
import logging
from datetime import datetime
import os
import re

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper_ultimate.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ì œ"""
    if not text:
        return ""
    return ' '.join(text.split()).strip()

def safe_get_text(element, default=""):
    """ì•ˆì „í•˜ê²Œ ìš”ì†Œì˜ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
    try:
        if element and element.count() > 0:
            return clean_text(element.inner_text())
    except:
        pass
    return default

def safe_get_attribute(element, attr, default=""):
    """ì•ˆì „í•˜ê²Œ ìš”ì†Œì˜ ì†ì„± ê°€ì ¸ì˜¤ê¸°"""
    try:
        if element and element.count() > 0:
            return element.get_attribute(attr)
    except:
        pass
    return default

def extract_search_result_info(result_box, page):
    """ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì˜ ê° êµìœ¡ê³¼ì • ë°•ìŠ¤ì—ì„œ ëª¨ë“  ì •ë³´ ì¶”ì¶œ"""
    info = {}

    try:
        # 1. ì¸ë„¤ì¼ ì´ë¯¸ì§€ URL
        thumbnail = result_box.locator('.curriculum__thumbnail img').first
        if thumbnail.count() > 0:
            img_src = thumbnail.get_attribute('src')
            if img_src and not img_src.endswith('no_img.gif'):
                info['ì¸ë„¤ì¼_ì´ë¯¸ì§€'] = img_src

        # 2. êµìœ¡ë¹„ ìœ ë£Œ/ë¬´ë£Œ
        charge_elem = result_box.locator('.change-ico-box .ico').first
        if charge_elem.count() > 0:
            charge_text = charge_elem.inner_text()
            info['êµìœ¡ë¹„_êµ¬ë¶„'] = charge_text

        # 3. êµìœ¡í˜•íƒœ ë°°ì§€ (ëŒ€ë©´/ì˜¨ë¼ì¸ ë“±)
        badge = result_box.locator('.curriculum__info--badge .badge').first
        if badge.count() > 0:
            badge_class = badge.get_attribute('class')
            badge_text = badge.inner_text()
            info['êµìœ¡í˜•íƒœ'] = badge_text

            # í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ë” ì •í™•í•œ êµ¬ë¶„
            if 'face' in badge_class:
                info['êµìœ¡í˜•íƒœ_êµ¬ë¶„'] = 'ëŒ€ë©´'
            elif 'live' in badge_class:
                info['êµìœ¡í˜•íƒœ_êµ¬ë¶„'] = 'ë¼ì´ë¸Œ'
            elif 'hybrid' in badge_class:
                info['êµìœ¡í˜•íƒœ_êµ¬ë¶„'] = 'í•˜ì´ë¸Œë¦¬ë“œ'
            elif 'e-learning' in badge_class:
                info['êµìœ¡í˜•íƒœ_êµ¬ë¶„'] = 'ì´ëŸ¬ë‹'
            elif 'bl' in badge_class:
                info['êµìœ¡í˜•íƒœ_êµ¬ë¶„'] = 'B/L'

        # 4. êµìœ¡ë¶„ì•¼ ì¹´í…Œê³ ë¦¬
        category_spans = result_box.locator('.curriculum__info--badge span').all()
        categories = []
        for span in category_spans:
            text = clean_text(span.inner_text())
            if text and not text.startswith('ëª¨ì§‘'):
                categories.append(text)
        if categories:
            info['êµìœ¡ë¶„ì•¼'] = ', '.join(categories)

        # 5. ëª¨ì§‘ìƒíƒœ
        recruit_status = result_box.locator('.curriculum__info--badge em').first
        if recruit_status.count() > 0:
            status_class = recruit_status.get_attribute('class')
            status_text = recruit_status.inner_text()
            info['ëª¨ì§‘ìƒíƒœ'] = status_text

            if 'yellow' in status_class:
                info['ëª¨ì§‘ìƒíƒœ_êµ¬ë¶„'] = 'ì§„í–‰ì¤‘'
            elif 'gray' in status_class:
                info['ëª¨ì§‘ìƒíƒœ_êµ¬ë¶„'] = 'ë§ˆê°'

        # 6. êµìœ¡ëŒ€ìƒ
        target_elem = result_box.locator('.curriculum__info--badge em.gray').first
        if target_elem.count() > 0:
            target_text = target_elem.inner_text()
            if 'ê³µë¬´ì›' in target_text or 'ë¯¼ê°„' in target_text:
                info['êµìœ¡ëŒ€ìƒ_í‘œì‹œ'] = target_text

        # 7. í”Œë«í¼ í˜¸í™˜ì„±
        pc_elem = result_box.locator('.info--pc')
        mobile_elem = result_box.locator('.info--mobile')
        sign_elem = result_box.locator('.info--sign')

        platforms = []
        if pc_elem.count() > 0:
            platforms.append('PC')
        if mobile_elem.count() > 0:
            platforms.append('Mobile')
        if sign_elem.count() > 0:
            platforms.append('ìˆ˜ì–´ì§€ì›')

        if platforms:
            info['ì§€ì›í”Œë«í¼'] = ', '.join(platforms)

        # 8. ë§›ë³´ê¸° ì˜ìƒ ë§í¬
        teaser = result_box.locator('.slide__link--teaser').first
        if teaser.count() > 0:
            info['ë§›ë³´ê¸°ì˜ìƒ'] = 'ìˆìŒ'

        # 9. ìƒì„¸ ì •ë³´ í…ìŠ¤íŠ¸ë“¤
        details = result_box.locator('.curriculum__info--detail p').all()
        for detail in details:
            text = detail.inner_text()
            if 'ì‹ ì²­ê¸°ê°„' in text:
                info['ê²€ìƒ‰ê²°ê³¼_ì‹ ì²­ê¸°ê°„'] = text.replace('ì‹ ì²­ê¸°ê°„ : ', '').strip()
            elif 'êµìœ¡ê¸°ê°„' in text:
                info['ê²€ìƒ‰ê²°ê³¼_êµìœ¡ê¸°ê°„'] = text.replace('êµìœ¡ê¸°ê°„ : ', '').strip()
            elif 'êµìœ¡ì‹œê°„' in text:
                info['ê²€ìƒ‰ê²°ê³¼_êµìœ¡ì‹œê°„'] = text.replace('êµìœ¡ì‹œê°„ : ', '').strip()
            elif 'ì‹ ì²­ì¸ì›' in text:
                info['ê²€ìƒ‰ê²°ê³¼_ì‹ ì²­í˜„í™©'] = text.replace('ì‹ ì²­ì¸ì›/ì •ì› : ', '').strip()

        # 10. onclick ì†ì„±ì—ì„œ ì½”ë“œ ì¶”ì¶œ
        link = result_box.locator('a').first
        if link.count() > 0:
            onclick = link.get_attribute('onclick')
            if onclick:
                match = re.search(r"btn_selectPaa0040\('([^']+)','([^']+)'\)", onclick)
                if match:
                    info['êµìœ¡ê³¼ì •ì½”ë“œ'] = match.group(1)
                    info['êµìœ¡ê·¸ë£¹ì½”ë“œ'] = match.group(2)

    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ê²°ê³¼ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")

    return info

def extract_detail_page_complete(page):
    """ìƒì„¸ í˜ì´ì§€ì—ì„œ ëª¨ë“  ì •ë³´ ì™„ì „ ì¶”ì¶œ"""
    data = {}

    try:
        # í˜„ì¬ URL ì €ì¥
        data['ìƒì„¸í˜ì´ì§€_URL'] = page.url

        # 1. í˜ì´ì§€ ì œëª© (ë‹¤ì–‘í•œ selector ì‹œë„)
        title_selectors = [
            'h3.tit',
            'h3.sub_cont_title_h3',
            '.page-title h3',
            '.content-title',
            'h3'
        ]

        for selector in title_selectors:
            elem = page.locator(selector).first
            if elem.count() > 0:
                title = clean_text(elem.inner_text())
                if title and len(title) > 2:
                    data['êµìœ¡ê³¼ì •ëª…'] = title
                    break

        # 2. ëª¨ë“  h4 ì„¹ì…˜ ì°¾ê¸° (êµìœ¡ì†Œê°œ, êµìœ¡ëª©í‘œ ë“±)
        h4_sections = page.locator('h4').all()
        for h4 in h4_sections:
            try:
                section_title = clean_text(h4.inner_text())

                # ë‹¤ìŒ í˜•ì œ ìš”ì†Œë“¤ì—ì„œ ë‚´ìš© ìˆ˜ì§‘
                content = page.evaluate(f"""
                    (() => {{
                        const h4 = Array.from(document.querySelectorAll('h4')).find(el => el.textContent.includes('{section_title}'));
                        if (!h4) return '';
                        let content = '';
                        let next = h4.nextElementSibling;
                        while (next && next.tagName !== 'H4') {{
                            if (next.tagName !== 'SCRIPT' && next.tagName !== 'STYLE') {{
                                content += next.innerText + ' ';
                            }}
                            next = next.nextElementSibling;
                        }}
                        return content.trim();
                    }})()
                """)

                if content:
                    # ì„¹ì…˜ë³„ë¡œ ì €ì¥
                    if 'êµìœ¡ì†Œê°œ' in section_title:
                        data['êµìœ¡ì†Œê°œ'] = clean_text(content)
                    elif 'êµìœ¡ëª©í‘œ' in section_title:
                        data['êµìœ¡ëª©í‘œ'] = clean_text(content)
                    elif 'í•™ìŠµë°©ë²•' in section_title:
                        data['í•™ìŠµë°©ë²•'] = clean_text(content)
                    elif 'í‰ê°€ë°©ë²•' in section_title:
                        data['í‰ê°€ë°©ë²•'] = clean_text(content)
                    elif 'ê°•ì‚¬' in section_title:
                        data['ê°•ì‚¬ì •ë³´'] = clean_text(content)
                    elif 'ë¬¸ì˜' in section_title:
                        data['ë¬¸ì˜ì²˜'] = clean_text(content)
                    else:
                        # ê¸°íƒ€ ì„¹ì…˜
                        data[f'ê¸°íƒ€_{section_title}'] = clean_text(content)[:500]

            except Exception as e:
                logger.debug(f"h4 ì„¹ì…˜ íŒŒì‹± ì˜¤ë¥˜: {e}")

        # 3. ëª¨ë“  í…Œì´ë¸” ë¶„ì„ (ë” ì •ë°€í•˜ê²Œ)
        tables = page.locator('table').all()
        logger.info(f"  í…Œì´ë¸” ìˆ˜: {len(tables)}")

        for idx, table in enumerate(tables):
            try:
                table_html = table.inner_html()
                table_text = table.inner_text()

                # ì‹ ì²­ì •ë³´ í…Œì´ë¸”
                if any(key in table_text for key in ['êµìœ¡ëŒ€ìƒ', 'ì‹ ì²­ê¸°ê°„', 'êµìœ¡ê¸°ê°„', 'êµìœ¡ë¹„']):
                    rows = table.locator('tr').all()
                    for row in rows:
                        ths = row.locator('th').all()
                        tds = row.locator('td').all()

                        for i in range(len(ths)):
                            if i < len(tds):
                                key = clean_text(ths[i].inner_text())
                                value = clean_text(tds[i].inner_text())
                                if key:
                                    # í‚¤ ì´ë¦„ ì •ê·œí™”
                                    key_normalized = key.replace('/', '_').replace(' ', '_')
                                    data[f'ì‹ ì²­_{key_normalized}'] = value

                # ìˆ˜ë£Œê¸°ì¤€ í…Œì´ë¸”
                elif any(key in table_text for key in ['ìˆ˜ë£Œ', 'ì¶œì„', 'ì‹œí—˜', 'ê³¼ì œ']):
                    # í…Œì´ë¸” êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
                    rows = table.locator('tr').all()

                    # í—¤ë”ê°€ ìˆëŠ” ê²½ìš°
                    if len(rows) >= 2:
                        headers = []
                        for th in rows[0].locator('th, td').all():
                            headers.append(clean_text(th.inner_text()))

                        # ë°ì´í„° í–‰
                        if len(rows) > 1:
                            values = []
                            for td in rows[1].locator('td').all():
                                values.append(clean_text(td.inner_text()))

                            if len(headers) == len(values):
                                for h, v in zip(headers, values):
                                    if h:
                                        data[f'ìˆ˜ë£Œ_{h}'] = v

                # êµìœ¡êµ¬ì„± í…Œì´ë¸”
                elif any(key in table_text for key in ['êµê³¼ëª©', 'ê°•ì‚¬', 'êµìœ¡ì¼', 'ì°¨ì‹œ']):
                    # í—¤ë” ì°¾ê¸°
                    headers = []
                    thead = table.locator('thead')
                    if thead.count() > 0:
                        headers = [clean_text(th.inner_text()) for th in thead.locator('th').all()]
                    else:
                        first_row = table.locator('tr').first
                        if first_row.count() > 0:
                            headers = [clean_text(th.inner_text()) for th in first_row.locator('th').all()]

                    if headers:
                        # ë°ì´í„° í–‰ ìˆ˜ì§‘
                        tbody = table.locator('tbody')
                        if tbody.count() > 0:
                            data_rows = tbody.locator('tr').all()
                        else:
                            all_rows = table.locator('tr').all()
                            data_rows = all_rows[1:] if len(all_rows) > 1 else []

                        curriculum_data = []
                        for row in data_rows:
                            cells = [clean_text(td.inner_text()) for td in row.locator('td').all()]
                            if cells and len(cells) == len(headers):
                                curriculum_data.append(dict(zip(headers, cells)))

                        if curriculum_data:
                            data['êµìœ¡êµ¬ì„±'] = json.dumps(curriculum_data, ensure_ascii=False)
                            data['êµìœ¡êµ¬ì„±_ê³¼ëª©ìˆ˜'] = len(curriculum_data)

                            # ì´ êµìœ¡ì‹œê°„ ê³„ì‚° ì‹œë„
                            total_hours = 0
                            for item in curriculum_data:
                                for key in ['ì‹œê°„', 'êµìœ¡ì‹œê°„', 'ì°¨ì‹œ']:
                                    if key in item:
                                        try:
                                            # ìˆ«ìë§Œ ì¶”ì¶œ
                                            hours = re.findall(r'\d+\.?\d*', item[key])
                                            if hours:
                                                total_hours += float(hours[0])
                                        except:
                                            pass
                            if total_hours > 0:
                                data['êµìœ¡êµ¬ì„±_ì´ì‹œê°„'] = total_hours

                # ì¶”ì²œêµìœ¡ê³¼ì • í…Œì´ë¸”
                elif 'ì¶”ì²œ' in table_text:
                    if 'ì¶”ì²œ êµìœ¡ê³¼ì •ì´ ì—†ìŠµë‹ˆë‹¤' in table_text:
                        data['ì¶”ì²œêµìœ¡ê³¼ì •'] = 'ì—†ìŒ'
                    else:
                        # ì¶”ì²œ ê³¼ì • íŒŒì‹±
                        headers = []
                        thead = table.locator('thead')
                        if thead.count() > 0:
                            headers = [clean_text(th.inner_text()) for th in thead.locator('th').all()]

                        if headers:
                            tbody = table.locator('tbody')
                            if tbody.count() > 0:
                                reco_data = []
                                for row in tbody.locator('tr').all():
                                    cells = [clean_text(td.inner_text()) for td in row.locator('td').all()]
                                    if cells and len(cells) == len(headers):
                                        reco_data.append(dict(zip(headers, cells)))

                                if reco_data:
                                    data['ì¶”ì²œêµìœ¡ê³¼ì •'] = json.dumps(reco_data, ensure_ascii=False)
                                    data['ì¶”ì²œêµìœ¡ê³¼ì •_ìˆ˜'] = len(reco_data)

                # ê¸°íƒ€ ì •ë³´ í…Œì´ë¸”
                else:
                    # í…Œì´ë¸”ì— ìœ ìš©í•œ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
                    if len(table_text) > 20 and len(table_text) < 2000:
                        # th-td ìŒìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì •ë³´ì„± í…Œì´ë¸”
                        rows = table.locator('tr').all()
                        for row in rows:
                            ths = row.locator('th').all()
                            tds = row.locator('td').all()

                            for i in range(len(ths)):
                                if i < len(tds):
                                    key = clean_text(ths[i].inner_text())
                                    value = clean_text(tds[i].inner_text())
                                    if key and value and len(key) < 30:
                                        data[f'ê¸°íƒ€ì •ë³´_{key}'] = value[:200]

            except Exception as e:
                logger.debug(f"í…Œì´ë¸” {idx} íŒŒì‹± ì˜¤ë¥˜: {e}")

        # 4. ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ íŒŒì¼ í™•ì¸
        download_links = page.locator('a[href*="download"], a[href*="file"]').all()
        if download_links:
            downloads = []
            for link in download_links[:5]:  # ìµœëŒ€ 5ê°œë§Œ
                href = link.get_attribute('href')
                text = clean_text(link.inner_text())
                if href and text:
                    downloads.append(f"{text}: {href}")
            if downloads:
                data['ë‹¤ìš´ë¡œë“œ_ìë£Œ'] = ', '.join(downloads)

        # 5. ë©”íƒ€ ì •ë³´ ìˆ˜ì§‘ (ìˆë‹¤ë©´)
        meta_info = page.locator('meta[property*="og:"], meta[name*="description"]').all()
        for meta in meta_info:
            prop = meta.get_attribute('property') or meta.get_attribute('name')
            content = meta.get_attribute('content')
            if prop and content:
                if 'description' in prop:
                    data['ë©”íƒ€_ì„¤ëª…'] = content[:200]
                elif 'image' in prop:
                    data['ë©”íƒ€_ì´ë¯¸ì§€'] = content

    except Exception as e:
        logger.error(f"ìƒì„¸ í˜ì´ì§€ ì „ì²´ íŒŒì‹± ì˜¤ë¥˜: {e}")
        data['íŒŒì‹±ì˜¤ë¥˜'] = str(e)[:200]

    return data

def scrape_course_complete(course_name, playwright_instance):
    """ë‹¨ì¼ êµìœ¡ê³¼ì • ì™„ì „ ìŠ¤í¬ë˜í•‘"""
    result = {
        'ì›ë³¸_êµìœ¡ê³¼ì •ëª…': course_name,
        'ìŠ¤í¬ë˜í•‘_ì‹œê°': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }

    browser = None
    try:
        # ë¸Œë¼ìš°ì € ì‹œì‘
        browser = playwright_instance.chromium.launch(
            headless=True,
            args=['--disable-blink-features=AutomationControlled']
        )
        page = browser.new_page()

        # 1. ê²€ìƒ‰ í˜ì´ì§€ ì´ë™
        logger.info(f"  ê²€ìƒ‰ ì‹œì‘: {course_name}")
        page.goto("https://edu.kohi.or.kr/pt/pa/paa/BD_paa0010l.do", timeout=30000)
        page.wait_for_load_state("networkidle", timeout=10000)

        # 2. ê²€ìƒ‰ ì‹¤í–‰
        search_input = page.locator("#planngCrseNm")
        search_input.fill(course_name)
        page.keyboard.press("Enter")

        # ê²€ìƒ‰ ê²°ê³¼ ëŒ€ê¸°
        time.sleep(3)

        try:
            page.wait_for_selector(".curriculum__box", timeout=5000)
        except:
            logger.debug("  ê²€ìƒ‰ ê²°ê³¼ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼")

        # 3. ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„
        results = page.locator(".curriculum__box").all()
        logger.info(f"  ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")

        if len(results) == 0:
            result['ìŠ¤í¬ë˜í•‘ê²°ê³¼'] = 'ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ'
            return result

        # ì²« ë²ˆì§¸ ê²°ê³¼ì—ì„œ ì •ë³´ ì¶”ì¶œ
        first_result = results[0]

        # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œ ëª¨ë“  ì •ë³´ ì¶”ì¶œ
        search_info = extract_search_result_info(first_result, page)
        result.update(search_info)

        # ê¸°ë³¸ ì œëª© ìˆ˜ì§‘
        title_elem = first_result.locator(".curriculum__info--title")
        if title_elem.count() > 0:
            result['ê²€ìƒ‰ê²°ê³¼_ì œëª©'] = clean_text(title_elem.inner_text())

        # 4. ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
        detail_link = first_result.locator("a").first

        if detail_link.count() > 0:
            # ìƒˆ í˜ì´ì§€ì—ì„œ ì—´ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëŒ€ê¸°
            try:
                with page.expect_navigation(timeout=30000, wait_until="domcontentloaded"):
                    detail_link.click()
            except:
                # navigationì´ ì—†ì„ ê²½ìš° ê·¸ëƒ¥ í´ë¦­
                detail_link.click()
                page.wait_for_load_state("domcontentloaded")

            time.sleep(2)  # í˜ì´ì§€ ì™„ì „ ë¡œë”© ëŒ€ê¸°

            # URL í™•ì¸
            current_url = page.url
            logger.info(f"  ìƒì„¸ í˜ì´ì§€ ì´ë™: {current_url}")

            # 5. ìƒì„¸ í˜ì´ì§€ì—ì„œ ì™„ì „í•œ ì •ë³´ ì¶”ì¶œ
            detail_data = extract_detail_page_complete(page)
            result.update(detail_data)

            # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
            parsed_fields = len([k for k in result.keys()
                               if k not in ['ì›ë³¸_êµìœ¡ê³¼ì •ëª…', 'ìŠ¤í¬ë˜í•‘_ì‹œê°']])

            if parsed_fields > 10:
                result['ìŠ¤í¬ë˜í•‘ê²°ê³¼'] = 'ì„±ê³µ'
                result['ìˆ˜ì§‘_í•„ë“œìˆ˜'] = parsed_fields
            elif parsed_fields > 5:
                result['ìŠ¤í¬ë˜í•‘ê²°ê³¼'] = 'ë¶€ë¶„ ì„±ê³µ'
                result['ìˆ˜ì§‘_í•„ë“œìˆ˜'] = parsed_fields
            else:
                result['ìŠ¤í¬ë˜í•‘ê²°ê³¼'] = 'ì •ë³´ ë¶€ì¡±'
                result['ìˆ˜ì§‘_í•„ë“œìˆ˜'] = parsed_fields

            logger.info(f"  ìˆ˜ì§‘ ì™„ë£Œ: {parsed_fields}ê°œ í•„ë“œ")
        else:
            result['ìŠ¤í¬ë˜í•‘ê²°ê³¼'] = 'ìƒì„¸ ë§í¬ ì—†ìŒ'

    except Exception as e:
        logger.error(f"  ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
        result['ìŠ¤í¬ë˜í•‘ê²°ê³¼'] = f'ì˜¤ë¥˜: {str(e)[:100]}'

    finally:
        if browser:
            browser.close()

    return result

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # CSV íŒŒì¼ ë¡œë“œ
    try:
        df = pd.read_csv(r"C:\KOHI\work.csv")
        course_names = df.iloc[:, 0].tolist()
        logger.info(f"ì´ {len(course_names)}ê°œ êµìœ¡ê³¼ì • ë¡œë“œ")
    except Exception as e:
        logger.error(f"CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    results = []

    with sync_playwright() as p:
        for idx, course_name in enumerate(course_names, 1):
            logger.info(f"\n[{idx}/{len(course_names)}] {course_name}")

            # ê° êµìœ¡ê³¼ì • ì™„ì „ ìŠ¤í¬ë˜í•‘
            result = scrape_course_complete(course_name, p)
            results.append(result)

            # ì§„í–‰ìƒí™© ì €ì¥ (10ê°œë§ˆë‹¤)
            if idx % 10 == 0:
                temp_df = pd.DataFrame(results)
                temp_df.to_csv(r"C:\KOHI\scraped_ultimate_temp.csv", index=False, encoding='utf-8-sig')
                logger.info(f"ì„ì‹œ ì €ì¥: {idx}ê°œ ì™„ë£Œ")

            # ì†ë„ ì¡°ì ˆ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if idx < len(course_names):
                time.sleep(1)

    # ìµœì¢… ê²°ê³¼ ì €ì¥
    if results:
        final_df = pd.DataFrame(results)
        final_df.to_csv(r"C:\KOHI\scraped_ultimate_final.csv", index=False, encoding='utf-8-sig')

        # í†µê³„ ì¶œë ¥
        logger.info("\n" + "="*50)
        logger.info("ğŸ‰ ì™„ë²½í•œ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        logger.info(f"ì´ ì²˜ë¦¬: {len(results)}ê°œ")

        if 'ìŠ¤í¬ë˜í•‘ê²°ê³¼' in final_df.columns:
            stats = final_df['ìŠ¤í¬ë˜í•‘ê²°ê³¼'].value_counts()
            for status, count in stats.items():
                logger.info(f"  {status}: {count}ê°œ")

        # ìˆ˜ì§‘ëœ í•„ë“œ í†µê³„
        all_columns = set(final_df.columns)
        logger.info(f"\nì´ ìˆ˜ì§‘ í•„ë“œ ì¢…ë¥˜: {len(all_columns)}ê°œ")

        # ì£¼ìš” í•„ë“œë³„ ìˆ˜ì§‘ë¥ 
        field_counts = {}
        for col in final_df.columns:
            non_null = final_df[col].notna().sum()
            if non_null > 0:
                field_counts[col] = non_null

        logger.info("\nğŸ“Š ì£¼ìš” ìˆ˜ì§‘ í•„ë“œ (ìƒìœ„ 20ê°œ):")
        for field, count in sorted(field_counts.items(), key=lambda x: x[1], reverse=True)[:20]:
            percentage = (count / len(results)) * 100
            logger.info(f"  {field}: {count}ê°œ ({percentage:.1f}%)")

        # ìƒˆë¡œ ì¶”ê°€ëœ í•„ë“œ ê°•ì¡°
        new_fields = [col for col in all_columns if any(keyword in col for keyword in
                     ['ì¸ë„¤ì¼', 'êµìœ¡í˜•íƒœ', 'ëª¨ì§‘ìƒíƒœ', 'êµìœ¡ë¹„_êµ¬ë¶„', 'í”Œë«í¼', 'êµìœ¡ëª©í‘œ',
                      'í•™ìŠµë°©ë²•', 'í‰ê°€ë°©ë²•', 'ê°•ì‚¬', 'êµìœ¡ë¶„ì•¼', 'ë§›ë³´ê¸°'])]

        if new_fields:
            logger.info(f"\nâœ¨ ìƒˆë¡œ ì¶”ê°€ëœ ì£¼ìš” í•„ë“œ:")
            for field in new_fields:
                count = final_df[field].notna().sum()
                if count > 0:
                    logger.info(f"  {field}: {count}ê°œ")

        logger.info(f"\nğŸ’¾ ìµœì¢… ê²°ê³¼ íŒŒì¼: C:\\KOHI\\scraped_ultimate_final.csv")

if __name__ == "__main__":
    main()